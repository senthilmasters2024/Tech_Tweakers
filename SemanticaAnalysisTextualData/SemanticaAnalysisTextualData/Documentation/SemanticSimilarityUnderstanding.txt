//Added on 10/01/2025
What is Semantic Similairy?

Semantic similarity refers to a measure of how similar two pieces of text are in terms of their meaning, rather than 
their exact words or syntax. It evaluates the degree to which two texts convey the same or related ideas, concepts, 
or context.

Semantic Similairy can be achieved with the help of embeddings because embedded values has to be given as input to any 
similarity algorithm to actualy find how closely related or unrelated.

https://www.nuget.org/packages/OpenAI/#how-to-generate-text-embeddings

Embedding helps to create a high vector values usually an float values as kind of number.

It helpts to find if  which two texts convey the same or related ideas, concepts, or context.

https://www.nuget.org/packages/OpenAI/#how-to-generate-text-embeddings - In this example, you want to create a trip-planning website that allows customers to write a prompt describing the 
kind of hotel that they are looking for and then offers hotel recommendations that closely match this description. 
To achieve this, it is possible to use text embeddings to measure the relatedness of text strings. 
In summary, you can get embeddings of the hotel descriptions, store them in a vector database, and 
use them to build a search index that you can query using the embedding of a given customer's prompt.

we will try to implement a different application to do basic comparison of texts from documents and try to create a visual representation
we will have to focus on measuring and comparing semantic relationships, leveraging examples with well-known entities and terms to 
highlight key insights.
we will have to think of basic application may be duplicate inputs detection system from text or document classification to find wheather it looks bills or invoice.

This is how sample response looks alike for text to embedding from Open AI embedding API , but the same think we will try
to implement via SDK
"embedding": [
        0.0023064255,
        -0.009327292,
        .... (1536 floats total for ada-002)
        -0.0028842222,
      ],

What is Embeddings:
https://platform.openai.com/docs/guides/embeddings

OpenAIâ€™s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:

Search (where results are ranked by relevance to a query string)
Clustering (where text strings are grouped by similarity)
Recommendations (where items with related text strings are recommended)
Anomaly detection (where outliers with little relatedness are identified)
Diversity measurement (where similarity distributions are analyzed)
Classification (where text strings are classified by their most similar label)
An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.


https://cookbook.openai.com/examples/semantic_text_search_using_embeddings  Example of some sample application using
embeddings such as find the type of reviews or context of review using embedding and some key as an input.

Data set used for the above example can be find on this site  https://cookbook.openai.com/examples/get_embeddings_from_dataset

To Create Tokens from the given text https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken

Some examples are given in the above link.

Cosine Similarity Sample Output With Vectors to understand similarity:

var vectorA = new double[] { 1, 2, 3 };
var vectorB = new double[] { 16,17,18 };
Cosine Similarity: 0.9746318461970762 

var vectorA = new double[] { 1, 2, 3 };
var vectorB = new double[] { 16,17,18 };
 
Cosine Similarity: 0.942886533797834

var vectorA = new double[] { 1, 2, 3 };
var vectorB = new double[] { 1, 2, 3 };

Cosine Similarity: 1

 var vectorA = new double[] { 1, 2, 3 };
 var vectorB = new double[] { -1, -2, -3 };
 
 Cosine Similarity: -1 ( Completely disimilar)
 
 var vectorA = new double[] { 1, 2, 3 };
var vectorB = new double[] { 100,20, 30 };

Cosine Similarity: 0.5782619234274095 (disimilar but might be related in broader terms like all numbers here are in positive side)

We will learn to plot this values like sample starting point for POC