Semantic Similarity Analysis of Textual Data
This project performs semantic analysis of textual data using .NET and C# in Visual Studio. The system processes text inputs at three levels: word, phrase, and document. Inputs are stored in designated folders and fetched for processing. The analysis is based on cosine similarity to measure text similarity.

a) Technology Stack

Programming Language: C#

Development Environment: Visual Studio

Framework: .NET Core/.NET Framework

Libraries:

System.IO for file handling

ML.NET or custom implementations for cosine similarity

Other NLP libraries if required

b)Processing Steps are as follows:

Data Fetching: The program scans the designated folder and reads text files.

Preprocessing:

Tokenization

Stopword removal (if necessary)

Normalization

Feature Extraction:

Vectorization of text using TF-IDF

Cosine Similarity Computation:

Converts text into vector space

Computes similarity scores between input texts

Output Generation:

Stores results in a structured format (e.g., JSON, CSV, or database)

c) Cosine Similarity Calculation is performed on the following basis:

Cosine similarity is used to measure the similarity between text vectors:

d(A, B) = (A • B) / (||A|| ||B||)

Where:

A and B are text vectors

A • B represents the dot product of the vectors

||A|| and ||B|| are the magnitudes of the vectors

Based on the researches, the following steps has been decided to follow to implement the task succesfully:

1) Apply the input (word, text or documents) into a preprocessing section. 
2) The output from the preprocessor is successfully applied to a section to create Embeddings using OpenAi's GPT model. 
3) From the high dimensional vector space, using a similarity model, the 'Semantic Similarity' will be extracted.
4) For Visualization part, we need to generate CSV file and then can generate Heatmaps or Network Graphs for understanding.

It is also founded that GPT doesnot provide numerical representation directly unlike other models including Sentence-BERT. Also , the Embeddings can be word or Sentence Embedding depending on the context. OpenAI also provides  endpoint (/v1/embeddings) for obtaining embeddings directly, this is quite efficient than GPTs completion endpoint.

The following are some of the easily available OpenAI's models used by GPT to generate vector space:

 .     text-similarity-ada-001
•	text-similarity-babbage-002
•	text-similarity-curie-001
•	text-similarity-davinci-002


Here, the  text-similarity-ada-001 is the popular one among others due to its simplicity and speed. The text-similarity-babbage-002 model is expensive than "ada models" . Its is preferred over "ada" for those tasks demanding a reasonable balance with less computationally such as keyword matching, basic semantic similarity analysis. The text-similarity-curie-001model is the one with better accuracy and performance as well as costly compared to ada. The model text-similarity-davinci-002 is the most advanced and hence the expensive. It can be integrated with, for applications which needs the highest precision with the ability to understand complex contextual relationships.  
 
As mentioned once the vector space is obtained by Embeddings, we can extract the similarity by using the below metric models: 

a) Cosine Similarity : This method focuses more on the orientation or direction of the vectors rather than their magnitude. This is useful when comparing high-dimensional embeddings. The value can be in the range of [-1,1] or [0,1] depending on our apllication. 

b) Euclidean Similarity: The absolute distances in the vector space is calculated if we are using Euclidean structure and this is  more sensitive to the magnitude of the vectors than simply the direction.

c) Jaccard Similarity: Once the  intersection and union is compared , the model focuses on calculating the similarity between two sets.

d) Pearson/Sparman Correlation Correlation Coefficient- This measures the linear relationship between two variables.

Of these above mentioned similarity metric models, the Cosine and Euclidean methods are the widely followed and easily adapted one due to less computational complexity.





